newdata <- data.frame(age=40, beauty=1.5, male=0, native=1)
res <- lm(eval~I(beauty-1.5)+I(age-40)+I(male-0)+I(native-1),
data=dat)
predict(res, newdata=newdata, interval="confidence",level=0.95)
#        fit      lwr      upr
# 1 4.129812 4.014937 4.244687
# Question 2
load(file='CPS1985.rda')
dat <- CPS1985
datam <- subset(dat, gender=="male")
datan <- subset(dat, gender=="female")
resm <- lm(wage~education+experience+I(experience^2), data=datam)
b <- coef(resm)
(aMax <- -b[3]/(2*b[4]))
aVec <- vector()
n <- nrow(datam)
for (i in 1:1000)
{
ind <- sample(n, size=n, replace=TRUE) # select the rows randomely
data <- datam[ind,] # get the new bootstrap sample
res2 <- lm(wage~education+experience+I(experience^2), data=data) # re-estimate with new sample
b2 <- coef(res2)
aVec[i] <- -b2[3]/(2*b2[4]) # store the aMax in a vector
}
(se <- sd(aVec)) #
(tstar <- qt(.975, nobs(resm)-3))
c(down = aMax-tstar*se, up = aMax+tstar*se)
# down.experience   up.experience
# 25.05566        45.72982
resn <- lm(wage~education+experience+I(experience^2), data=datan)
b <- coef(resn)
(aMax <- -b[3]/(2*b[4]))
aVec <- vector()
n <- nrow(datan)
for (i in 1:1000)
{
ind <- sample(n, size=n, replace=TRUE) # select the rows randomely
data <- datan[ind,] # get the new bootstrap sample
res2 <- lm(wage~education+experience+I(experience^2), data=data) # re-estimate with new sample
b2 <- coef(res2)
aVec[i] <- -b2[3]/(2*b2[4]) # store the aMax in a vector
}
(se <- sd(aVec)) #
(tstar <- qt(.975, nobs(resn)-3))
c(down = aMax-tstar*se, up = aMax+tstar*se)
# down.experience   up.experience
#       -430.8293        539.6558
library(AER)
library(stargazer)
library(car)
data("GrowthDJ")
dat <- GrowthDJ
# removing outlier
dat$gdp60[56] # this is the outlier of the linear regression
dat <- dat[-c(56), ]
# a)
res <- lm(gdpgrowth~gdp60+I(gdp60^2), data=dat)
linearHypothesis(res, "I(gdp60^2)=0")
se <- sqrt(diag(vcov(res)))
t_test <- res$coefficients[3]/se[3]
# -2.122037
(pt(t_test, length(dat$oil) - 2))*2
# 0.03592589
f <- function(x){
predict(res, data.frame(gdp60 = x))
}
c(f(2000), f(3000), f(4000), f(5000), f(6000), f(7000), f(8000))
# 4.285420 4.428507 4.480613 4.441735 4.311874 4.091031 3.779205
plot(gdpgrowth~gdp60, data=dat)
curve(f(x), add=TRUE)
# the convergence of gdpgrowth only starts after a certain level of
# gdp60, around 5000
# the estimated average growth is maximized around 4072 gdp60 and the maximum
# average growth is 4.48
# test the Solow prediction is harder in this scenario because
# the Solow prediction is "monotone decreasing" that is more gdp60 means less growth,
# whereas our model/regression is not "monotone", in other words, there is a part where the gdpgrowth is
# decreasing and another part that is increasing, thus our model
# is not always decreasing, which makes it harder to compare with the Solow prediction.
# We would have to ponder for example if the time that the model/regression is decreasing
# is much more than it is increasing.
# b)
res0 <- lm(gdpgrowth~gdp60, data=dat)
res1 <- lm(gdpgrowth~gdp60+literacy60, data=dat)
res2 <- lm(gdpgrowth~gdp60+literacy60+invest, data=dat)
res3 <- lm(gdpgrowth~gdp60+literacy60+invest+oecd, data=dat)
# there is clearly a large difference when we add the literacy60, since the
# the significance of the slope of gdp60 increases a lot (the p-value of the null
# hyhphotesis that slope is zero decreased a lot).
# Note as well that the literacy60 is quite significant in res1,
# but when we add invest and oecd it's p-value increases a lot
# Note as well that the oecd variable doesn't seem very significant, since it's p-value is quite
# large, also note that when we add the oecd variable it increases the gdp60 p-value making
# it less signifcant
get_p_value <- function(res){
se <- sqrt(diag(vcov(res)))
t_test <- res$coefficients[2]/se[2]
# -1.314309. same value as the t value of the summary output
return (pt(t_test, length(dat$oil) - 2))
}
c(get_p_value(res0), get_p_value(res1), get_p_value(res2), get_p_value((res3)))
# 0.1002281283 0.0026753698 0.0003107753 0.0059098432
# just like we saw in assignment3, the gdp60 significance increases
# a lot when we restrict our models to scenarios where variables
# such as literacy60, invest are constant. Thus the Solow prediction
# becomes more accurate under those scenarios.
c(summary(res0)$r.squared, summary(res0)$adj.r.squared)
# [1] 0.014456376 0.005734751
c(summary(res1)$r.squared, summary(res1)$adj.r.squared)
# [1] 0.09992964 0.08156086
c(summary(res2)$r.squared, summary(res2)$adj.r.squared)
# [1] 0.2522361 0.2291094
c(summary(res3)$r.squared, summary(res3)$adj.r.squared)
# [1] 0.2801016 0.2501058
# according to both r squared in adj rsquared the best model
# is res3. Since the number of samples is quite high, compared to the
# number of parameters, the adjusted rsquared is quite similar to
# the values of the raw r squared.
# r squared
f4 <- function(x){
x*res2$coefficients[2] + mean(dat$literacy60, na.rm=TRUE)*res2$coefficients[3] + mean(dat$invest, na.rm=TRUE)*res2$coefficients[4] + res2$coefficients[1]
}
plot(gdpgrowth~gdp60, data=dat)
curve(f4(x), add=TRUE)
# the reason why the other three models have less observations is because since the
# other three models regress over more parameters, more rows will be ignored
# because of the presence of NA, thus less rows will be used for the regression
# Yes this a problem, specially if there are many NA because than the more parameters
# we add the less observations we will have to do the regression
#c)
res <- lm(gdpgrowth~gdp60+I(gdp60^2)+literacy60+invest, data=dat)
linearHypothesis(res, "I(gdp60^2)=0")
se <- sqrt(diag(vcov(res)))
t_test <- res$coefficients[3]/se[3]
# -0.4888696
(pt(t_test, length(dat$oil) - 2))*2
# 0.6258414
# thus the p-value is much larger compared to a), thus we have less confidence that the quadratic term
# is significant compared to a)
f <- function(x){
x*res$coefficients[2] +x*x*res$coefficients[3] + mean(dat$literacy60, na.rm=TRUE)*res$coefficients[4] + mean(dat$invest, na.rm=TRUE)*res$coefficients[5] + res$coefficients[1]
}
c(f(2000), f(3000), f(4000), f(5000), f(6000), f(7000), f(8000))
# 4.367279 4.147428 3.905608 3.641821 3.356065 3.048342 2.718650
# clearly the gdpgrowth goes down much more faster compared to a)
plot(gdpgrowth~gdp60, data=dat)
curve(f(x), add=TRUE)
# unlike in a) the convergence of gdpgrowth happens for all values of gdp60
# the estimated average growth is maximized at gdp60=0
# test the Solow prediction is much easier in this scenario compared to a) because
# the Solow prediction is "monotone decreasing" that is more gdp60 means less growth, but
# and that is exactly the trend that our model/regresion in c) follows, thus
# our regression in c) is compatible with the Solow prediction
library(AER)
library(stargazer)
library(car)
library(wooldridge)
data('k401ksubs')
dat <- k401ksubs
# i)
dat <- subset(dat, marr==0 & fsize > 1 & male==0)
res <- lm(nettfa~inc+age, data = dat)
(summary(res))
# Call:
#   lm(formula = nettfa ~ inc + age, data = dat)
#
# Residuals:
#   Min      1Q  Median      3Q     Max
# -122.66  -11.12   -2.28    5.46  957.12
#
# Coefficients:
#   Estimate Std. Error t value Pr(>|t|)
# (Intercept) -38.08715    5.29517  -7.193 1.13e-12 ***
#   inc           1.06972    0.08815  12.135  < 2e-16 ***
#   age           0.50835    0.11938   4.258 2.22e-05 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#
# Residual standard error: 38.25 on 1170 degrees of freedom
# Multiple R-squared:  0.133,	Adjusted R-squared:  0.1315
# F-statistic: 89.73 on 2 and 1170 DF,  p-value: < 2.2e-16
stargazer(res, type="text")
# ===============================================
#   Dependent variable:
#   ---------------------------
#   nettfa
# -----------------------------------------------
#   inc                          1.070***
#   (0.088)
#
# age                          0.508***
#   (0.119)
#
# Constant                    -38.087***
#   (5.295)
#
# -----------------------------------------------
#   Observations                   1,173
# R2                             0.133
# Adjusted R2                    0.132
# Residual Std. Error     38.247 (df = 1170)
# F Statistic          89.733*** (df = 2; 1170)
# ===============================================
#   Note:               *p<0.1; **p<0.05; ***p<0.01
# one year more of age, increases the financial wealth by in average 500 dollars
# a thousand more dollars of income increases in average the wealth by 1000 dollars
# I am surprised that in average people only save 500 dollars every year,
# since their wealth increases in average by only 500 dollars
# ii)
# the intercept predicts that if a person has 0 years and no income it would have
# a financial wealth of -38 thousand dollars. This estimate is clearly wrong,
# because when you are born your wealth is 0 (if you don't count the wealth of)
# your parents
# the incorrect intercept is a result from the fact that few samples have
# small age and no income.
# iii)
dat$newY = dat$nettfa -0.8*dat$age
res1 <- lm(newY~inc+age, data = dat)
(b1 <- coef(res1))
(s1 <- sqrt(diag(vcov(res1))))
(t1 <- b1/s1)
qt(.975, nobs(res1)-3)
(pt(t1[3], nobs(res1) - 3))
# we can't reject with 1% significance
# iv)
res2 <- lm(nettfa~inc, data=dat)
# both values are very similar, because income and age are somewhat independent
# after you reach a certain age. Thus after you remove the age from the regression
# their effects don't get captured by the income
# v)
linearHypothesis(res, c("age=0.0", "inc=0.0"))
#
# Res.Df     RSS Df Sum of Sq      F    Pr(>F)
# 1   1172 1974019
# 2   1170 1711494  2    262525 89.733 < 2.2e-16 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# we can easily reject the null hyphotesis at 5% since the p-value was less
# than 2.2e-16
# Question 2)
# i)
data('htv')
dat <- htv
dat <- subset(dat, west==1)
res <- lm(educ~motheduc+fatheduc+abil+I(abil^2), data =dat)
summary(res)
# Call:
#   lm(formula = educ ~ motheduc + fatheduc + abil + I(abil^2), data = dat)
#
# Residuals:
#   Min      1Q  Median      3Q     Max
# -4.4458 -1.4620 -0.0741  1.1566  5.9833
#
# Coefficients:
#   Estimate Std. Error t value Pr(>|t|)
# (Intercept)  7.96622    0.86004   9.263  < 2e-16 ***
#   motheduc     0.22266    0.07434   2.995 0.003083 **
#   fatheduc     0.09390    0.04507   2.084 0.038447 *
#   abil         0.34157    0.09271   3.684 0.000294 ***
#   I(abil^2)    0.07606    0.02398   3.171 0.001753 **
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#
# Residual standard error: 1.942 on 203 degrees of freedom
# Multiple R-squared:  0.3588,	Adjusted R-squared:  0.3462
# F-statistic:  28.4 on 4 and 203 DF,  p-value: < 2.2e-16
stargazer(res, type="text")
# ==============================================
#   Dependent variable:
#   ---------------------------
#   educ
# -----------------------------------------------
#   motheduc                     0.223***
#   (0.074)
#
# fatheduc                      0.094**
#   (0.045)
#
# abil                         0.342***
#   (0.093)
#
# I(abil2)                     0.076***
#   (0.024)
#
# Constant                     7.966***
#   (0.860)
#
# -----------------------------------------------
#   Observations                    208
# R2                             0.359
# Adjusted R2                    0.346
# Residual Std. Error      1.942 (df = 203)
# F Statistic           28.399*** (df = 4; 203)
# ===============================================
#   Note:               *p<0.1; **p<0.05; ***p<0.01
linearHypothesis(res, "I(abil^2)=0")
# the p-value of this hyphotesis is less sthan 1%, thus we can easily
# reject it
# ii)
linearHypothesis(res, "motheduc=fatheduc")
# p-value is 21%, thus it is not easy to reject the null hyphotesis
# iii)
res1 <- lm(educ~motheduc+I(motheduc+fatheduc)+abil+I(abil^2), data = dat)
summary(res1)$coefficients[,4]
# (Intercept)               motheduc I(motheduc + fatheduc)
# 2.932357e-17           2.101218e-01           3.844695e-02
# abil              I(abil^2)
# 2.939591e-04           1.752787e-03
# just as before the p-value remains 21%, and it can't easily be rejected
# iv)
linearHypothesis(res, "motheduc=fatheduc")
# p-value is 21%, thus it is not easy to reject the null hyphotesis
# v)
res2 <- lm(educ~tuit18+tuit17+motheduc+fatheduc+abil+I(abil^2), data =dat)
linearHypothesis(res2, c("tuit18=0.0", "tuit17=0.0"))
# we can reject the null hyphotesis with significance 5%,
# thus yes they are jointly significant
# vi)
(cor(dat$tuit17, dat$tuit18))
# [1] 0.9844963
# it is preferred to use the average because when you have multicolinearity
# in a model, this reduces drastically the precision of the estimates
res3 <- lm(educ~I((tuit18+tuit17)/2)+motheduc+fatheduc+abil+I(abil^2), data =dat)
# the coefficient of the average is equal to the sum of the coefficients of each tuitiion
# year.
# vii) it doesn't make sense when interpreted from a casual perspective,
# since the increase of unit of tuition increases the education level by 0.08.
# Because more expensive tuition makes sense that the education level would be
# lower since, education becomes more expesive and thus less people would want
# to educate
load(file='TeachingRatings.rda')
dat <- TeachingRatings
# Q 1)
res <- lm(eval~beauty+I(beauty^2), data=dat)
# we shouldn't add the beauty squared factor, because we can't reject the null
# hyphotesis that its slope is not zero
# one more unit of beauty increases in average the evaluation by 0.14
# one more unit of beauty^2 decreases in average the evaluation by 0.03
# Q 2)
m <- mean(dat$beauty)
ape <- lm(eval~beauty+I(beauty^2-2*m*beauty), data=dat)
(coef(ape)[2])
# 0.1467102
# Q 3)
v <- vcov(res)
m <- mean(dat$beauty)
s <- sqrt(v[2,2]+4*m^2*v[3,3]+4*m*v[2,3])
b <- coef(res)
ape <- b[2]+2*b[3]*mean(dat$beauty)
crit <- qt(.975, res$df)
c(ape-s*crit, ape+s*crit)
# Q 4)
ape <- lm(eval~beauty+I(beauty^2-2*m*beauty), data=dat)
confint(ape, "beauty")
s <- sqrt(vcov(ape)[2,2])
b <- coef(ape)
df <- res$df
tstar <- qt(.975, df)
# The lower bound
low <- b[2] - tstar*s
low
## 0.07752359
# the upper bound
up <- b[2] + tstar*s
up
# 0.2158968
# Q 5)
res <- lm(eval~beauty+age+I(age*beauty), data=dat)
# one more unit of beauty decreases the evaluation in average by
# 0.33
# one more unit of ages increases the evaluation by a very small amount
# one more unit of age*beauty increases the evaluation by 0.01
# it seems that the age is not significant, since the p-value
# is quite large. But the interaction term is quite significant,
# which means that age itself is not a predictor but age times beauty
# but in fact it is not trivial in a first analysis if the interaction
# is significant because one depends of the other or if only because beauty
# is signifcant
# Q 6)
m <- mean(dat$age)
ape <- lm(eval~beauty+age+I(beauty*age-m*beauty), data=dat)
(coef(ape)[2])
# 0.1517305
# Q 7)
v <- vcov(res)
m <- mean(dat$beauty)
s <- sqrt(v[2,2]+4*m^2*v[3,3]+4*m*v[2,3])
b <- coef(res)
ape <- b[2]+2*b[3]*mean(dat$beauty)
crit <- qt(.975, res$df)
c(ape-s*crit, ape+s*crit)
# -0.63309918 -0.04522587
# Q 8)
ape <- lm(eval~beauty+age+I(beauty*age-m*beauty), data=dat)
confint(ape, "beauty")
s <- sqrt(vcov(ape)[2,2])
b <- coef(ape)
df <- res$df
tstar <- qt(.975, df)
# The lower bound
low <- b[2] - tstar*s
low
## -0.6330992
# the upper bound
up <- b[2] + tstar*s
up
# -0.04522587
# Q 9)
dat$male <- as.numeric(dat$gender == "male")
dat$native <- as.numeric(dat$native == "yes")
res1 <- lm(eval~beauty+age+I(beauty*age), data=dat)
summary(res1)$adj.r.squared
# [1] 0.05123299
res2 <- lm(eval~beauty+age+I(age^2)+I(beauty*age), data=dat)
summary(res2)$adj.r.squared
# [1] 0.04921442
res3 <- lm(eval~beauty+age+I(beauty*age)+I(beauty^2), data=dat)
summary(res3)$adj.r.squared
# [1] 0.0493375
res4 <- lm(eval~beauty+age+I(beauty*age)+I(beauty^2), data=dat)
summary(res4)$adj.r.squared
# [1] 0.0493375
res5 <- lm(eval~beauty+age+I(beauty*age)+I(male), data=dat)
summary(res5)$adj.r.squared
# [1] 0.08563304
res6 <- lm(eval~beauty+age+I(beauty*age)+I(male)+I(native), data=dat)
summary(res6)$adj.r.squared
# [1] 0.1083137
# res6 is the linear regression with the best adjusted r squared
# all parameters of this linear regression are "useful" in the
# sense that they all have a high signifcance except the age
# one more unit of beauty decreases in average the evaluation by 0.4
# one more unit of beauty*age increases the evaluation by 0.11
# being male increases the evaluation by 0.22 in average
# being native increases the evaluation by 0.36 in average
# Q 10)
res <- lm(eval~I(beauty-1.5)+I(age-40)+I(male-1)+I(native-1),
data=dat)
res2 <- lm(eval~I(beauty)+I(age)+I(male)+I(native),
data=dat)
(p <- coef(res)[1])
#    4.340116
crit <- qt(.975, res$df)
s <- sqrt(vcov(res)[1,1])
c(p-s*crit, p+s*crit)
# 4.218323    4.461909
# Q 11)
res <- lm(eval~I(beauty-1.5)+I(age-40)+I(male-0)+I(native-1),
data=dat)
(p <- coef(res)[1])
#    4.129812
crit <- qt(.975, res$df)
s <- sqrt(vcov(res)[1,1])
c(p-s*crit, p+s*crit)
# 4.014937    4.244687
# for females the average prediction and the confidence interval
# are both smaller than with male
# Q 12)
newdata <- data.frame(age=40, beauty=1.5, male=1, native=1)
res <- lm(eval~I(beauty-1.5)+I(age-40)+I(male-1)+I(native-1),
data=dat)
predict(res, newdata=newdata, interval="confidence",level=0.95)
# fit      lwr      upr
# 1 4.340116 4.218323 4.461909
newdata <- data.frame(age=40, beauty=1.5, male=0, native=1)
res <- lm(eval~I(beauty-1.5)+I(age-40)+I(male-0)+I(native-1),
data=dat)
predict(res, newdata=newdata, interval="confidence",level=0.95)
#        fit      lwr      upr
# 1 4.129812 4.014937 4.244687
# Question 2
load(file='CPS1985.rda')
dat <- CPS1985
datam <- subset(dat, gender=="male")
datan <- subset(dat, gender=="female")
resm <- lm(wage~education+experience+I(experience^2), data=datam)
b <- coef(resm)
(aMax <- -b[3]/(2*b[4]))
aVec <- vector()
n <- nrow(datam)
for (i in 1:1000)
{
ind <- sample(n, size=n, replace=TRUE) # select the rows randomely
data <- datam[ind,] # get the new bootstrap sample
res2 <- lm(wage~education+experience+I(experience^2), data=data) # re-estimate with new sample
b2 <- coef(res2)
aVec[i] <- -b2[3]/(2*b2[4]) # store the aMax in a vector
}
(se <- sd(aVec)) #
(tstar <- qt(.975, nobs(resm)-3))
c(down = aMax-tstar*se, up = aMax+tstar*se)
# down.experience   up.experience
# 25.05566        45.72982
resn <- lm(wage~education+experience+I(experience^2), data=datan)
b <- coef(resn)
(aMax <- -b[3]/(2*b[4]))
aVec <- vector()
n <- nrow(datan)
for (i in 1:1000)
{
ind <- sample(n, size=n, replace=TRUE) # select the rows randomely
data <- datan[ind,] # get the new bootstrap sample
res2 <- lm(wage~education+experience+I(experience^2), data=data) # re-estimate with new sample
b2 <- coef(res2)
aVec[i] <- -b2[3]/(2*b2[4]) # store the aMax in a vector
}
(se <- sd(aVec)) #
(tstar <- qt(.975, nobs(resn)-3))
c(down = aMax-tstar*se, up = aMax+tstar*se)
# down.experience   up.experience
#       -430.8293        539.6558
quit()
